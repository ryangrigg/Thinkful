{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf #needed for models in this script\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('html', True) #see the dataframe in a more user friendly manner\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Classifier Performance Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the unit we've been splitting our data into training, test, and validation sets. Let's take a moment and discuss why this is necesary. By now you can probably see that learning an estimator and testing that estimator's performance on the same data is a methodological mistake. It's like if a professor administered a test with the exact same questions as the practice test. All a student would have to do to get 100% would be to memorize all the solutions to the practice test; they wouldn't acutally have to learn anything. If you test your estimator on the data used to train it, it knows all the answers, and thus can achieve a perfect score, even though it very well could fail to predict anything on new data. This is called overfitting. Predicting on never-before-seen data is kind of the whole point, so knowing how our estimator performs on data it has already seen isn't really useful.\n",
    "\n",
    "Holding out a subset of your data for testing, i.e., excluding a subset of your data from your training set, gives you some never-before-seen data to test your estimator's performance. The scikit-learn library has a train_test_split helper function to randomly split data into training and test sets.\n",
    "\n",
    "When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and we can't make claims about how it will generalize (i.e., how it will perform) on never-before-seen data.\n",
    "\n",
    "To resolve this problem, we can hold out yet another subset of our data for validation. Training proceeds on the training set, evaluation is done on the validation set, and when it seems like we have a good model, we can perform our final evaluation on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](files/evalc1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Try it!</b>\n",
    "* Use the cross_validation.train_test_split() helper function to split the Iris dataset into training and test sets, holding out 40% of the data for testing. How many points do you have in your training set? In your test set?\n",
    "* Fit a linear Support Vector Classifier to the training set and evaluate its performance on the test set. What is the score? How does it compare to the score in the Support Vector Machine lesson?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "************ To Complete ****************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more data we set aside for testing and validation, the less data we have for training, and this will negatively impact estimator performance. To resolve this problem, we can use cross validation (see lesson 4.1.5) to \"recycle\" data over different folds. In this assignment, we're going to implement 5-fold cross-validation on the Iris dataset to train and test a Support Vector Machine classifier.\n",
    "\n",
    "* Compute the 5-fold cross-validation score of the SVC from the last assignment.\n",
    "* Compute the mean score and the standard deviation of the scores.\n",
    "\n",
    "As the sklean documentation notes, the default score computed at each cross-validation iteration is the estimator's accuracy. We could tell it to return the F1 score, precision, or recall instead.\n",
    "\n",
    "* How do the accuracy scores compare to the F1 scores for this dataset?\n",
    "\n",
    "For more detail on the different scoring parameters, see: http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "******************** To Complete ********************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\"How do I know if my model fits the data well?\"</b>\n",
    "\n",
    "This is an important question that you should be asking yourself. A lot. And while it may feel like a silly thing to ask, evaluating the fit of a model is not as straightforward as it may seem. Many evaluation metrics have been developed to assess the fit of models, but it's important to keep in mind that each of these metrics was designed with a subset of models in mind, and probably evaluates different aspects of a model than another metric as a result.\n",
    "\n",
    "A really good example of this are the metrics used to evaluate the fit of binary classification models. One way of evaluating the fit of a binary model is with a contingency table. Contingency tables break up the predicted classes against the actual classes like so:\n",
    "\n",
    "![](files/evalCPer.jpg)\n",
    "\n",
    "In this example contingency table, 3 observations were correctly predicted and 3 observations were incorrectly predicted. This gives us a percentage correctly predicted (PCP) score (another metric for evaluating binary classifiers) of 50%. As you might recal from the Classification, Regression, and Prediction lesson, that these metrics depend on the probability threshold we set to coerce continuous predictions to categorical predictions. If we change that threshold, our contingency table and out PCP score could change too. Additionally, we can lose information about the continuous predictions of the model itself. There's a difference between classifying an observation as 1 because the predicted probability was 0.51 or because the predicted probability was 0.99, but there's no distinction between these two cases in a contingency table or a PCP score.\n",
    "\n",
    "A Receiver Operating Characteristic (ROC) curve is another way of evaluating the fit of a binary model. The ROC curve shows, graphically, the tradeoff between false-positives and false-negatives as the probability threshold for classifying observations is varied. The overall predictive power of a model is captured in the area under the ROC curve, termed the AUC (\"Area Under Curve\") score, and falls somewhere between 0 and 1. A model that performs no better than a coin toss would have an AUC score of 0.5. The limitation of the ROC curve in evaluating a model is that the shape of the curve doesn't tell us a lot about our model. All we have to go on is the area underneath it. How would we compare a ROC curve that's high on the left side with a ROC curve that's high on the right side if they both have the same AUC?\n",
    "\n",
    "* Do a little research on Brier Scores, Expected Percentage of Correct Predictions, and Pseudo R2 measures. What features of binary model fit do they capture? What do they miss?\n",
    "\n",
    "The best way to evaluate a model isn't written in stone anywhere, but computing multiple metrics tells you more than one single metric will. Computing these metrics is part of the practice of performing model diagnostics, and is useful not only for you as you're testing hypotheses and tuning models, but also for others to see your iterative modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
